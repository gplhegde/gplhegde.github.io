---
layout: post
author : Gopalakrishna Hegde
title : Matrices and linear transformations
date:   2017-07-28
---

# Matrices and linear transformations
Matrices are convenient mathematical constructs that enable us to represent and manipulate data mathematically.  In this article , I would like to talk about some basic mathematical concepts about matrices and their geometrical interpretation. I will try to avoid formal mathematical definitions and notations. The idea here is to give some intuitive understanding on how the math behind matrices is connected to real-world examples. This is not a self-contained article by any means. You may want to refer to the [previous](http://gplhegde.github.io/2017/07/15/vectors.html) article on vectors and vector spaces  if you are not familiar with those concepts. It will help to get better continuity if you do so.

In the previous post on vectors, we talked about representing physical entities that contain more than one parameters such as BMI as vectors.  In reality, we will have many such vectors each representing a data point; say BMI of a million different people. Since all these vectors are related, in the sense that they all represent a particular entity (BMI in this example), we can stack all these vectors side-by-side and thus creating a matrix. For example,  consider three data points (vectors) that represent height and weight of three different people and create a matrix $$M$$ as shown below.

$$
v = \begin{bmatrix}height\\weight\end{bmatrix}\hspace{2cm} v_1 = \begin{bmatrix}1.5\\52\end{bmatrix}, v2 = \begin{bmatrix}1.65\\65\end{bmatrix}, v_3 = \begin{bmatrix}1.8\\80\end{bmatrix}\\
M = \begin{bmatrix}v_1 \hspace{1cm}v_2\hspace{1cm}v_3\end{bmatrix} = \begin{bmatrix}1.5&1.65&1.8\\52&65&80\end{bmatrix}
$$

Here the matrix $$M$$ has 2 `rows` and 3 `columns` or it is of dimension $$2 \times 3$$.  In this example, each column of the matrix $$M$$ is a 2-D vector with height and weight being its two dimensions. We already know that a 2-D vector represents a point in the 2-D plane. Lets call this plane as Height-Weight plane OR $$H - W$$  plane.

![Height - Weight plane representing above 3 vectors]({{ site.url }}/assets/matrix/h_w_plane.png)

This is nothing but the $$X-Y$$ plane that we are familiar with. I just gave different names for the axes to give them a real-world significance. From vector space perspective, this plane is a 2-D vector space whose basis vectors are,

$$
h = \begin{bmatrix}1\\0\end{bmatrix} \hspace{2cm} w = \begin{bmatrix}0\\1\end{bmatrix}
$$

In this formulation,  the number of rows ($$m$$) represent the `dimension` of the space and number of columns represents number of vectors of that dimension (is of less significance since you can add more such vectors without affecting the property of the space).

We know that each vector (data point) in the vector space (interchangeably referred as space in this article) can be represented as linear combination of its basis vectors. Meaning, any 2-D vector $$v$$ can be represented as below.

$$
v = c_1*h + c_2 * w
$$

Now, can you guess what are the values of $$c_1$$ and $$c_2$$ for the vector $$v_1$$ in the above figure? Since the basis is orthonormal, the scaling factors $$c_1$$ and $$c_2$$ are in fact  vector $$v_1$$'s components themselves.

$$
v = c_1 * \begin{bmatrix}1\\0\end{bmatrix} + c_2 * \begin{bmatrix}0\\1\end{bmatrix}\\
v_1 = 1.5 * \begin{bmatrix}1\\0\end{bmatrix}  + 52 * \begin{bmatrix}0\\1\end{bmatrix}
$$

where $$v_h$$ and $$v_w$$ are $$h$$ and $$w$$ components of the vector $$v$$ in our $$H-W$$  plane.

Now, how about stacking those two basis vectors into another matrix, say matrix $$B$$ as below.

$$
B = \begin{bmatrix}1&0\\0&1\end{bmatrix}
$$

This special matrix's name is `identity matrix` and we will learn more about this when we talk about transformations later in this article.

There are several operations like addition and multiplication defined on matrices. You might be already knowing about these operations. The operation of our interest in this article is matrix multiplication which is not so intuitive. So, lets talk about that now.

# Matrix multiplication - linear transformation

You may be already knowing the steps to multiply two matrices. You perform element-wise multiplication and addition(called as dot-product ) between a row of the first matrix and a column of the second matrix. To do that,  it is a fundamental requirement that the number of columns in the first matrix must be same as the number of rows in the second matrix. Have you ever given a thought as to why the matrix multiplication is defined like that? We will explore about that now.

Matrix - matrix multiplication or matrix - vector (vector is also a matrix with one column) multiplication is a linear transformation. What does that even mean? Lets get back to our  $$H- W$$ plane example where we have collected height and weights of different people and nicely arranged in the form of a matrix $$M$$. Now lets say that a person's heart rate ($$R$$) and blood pressure level ($$P$$)  is directly dependent on his height and weight (yes, I am making this up!). We can transform height and weight vector into another vector whose elements are R and P..! We can do that by multiplying a vector in the $$H-W$$ plane by a matrix called the   transformation matrix $$T$$. We can create (learn) such a matrix given enough number of data points using machine learning techniques which we will get into in this post.

![image on transformation from one space to other]({{ site.url }}/assets/matrix/linear_transformation.png)

Lets say one such transformation matrix is, $$T = \begin{bmatrix}1&2\\3&2\end{bmatrix}$$. The columns of the transformation matrix are actually the transformed $$h$$ and $$w$$ axis of the original space. Meaning, the basis vector $$h = \begin{bmatrix}1\\0\end{bmatrix}$$ got transformed into $$r = \begin{bmatrix}1\\3\end{bmatrix}$$ and $$w = \begin{bmatrix}0\\1\end{bmatrix}$$ got transformed into $$p = \begin{bmatrix}2\\2\end{bmatrix}$$ when we applied the transformation $$T$$.  Lets call the transformed space as $$R-P$$ space with $$r$$ and $$p$$ being its basis vectors. Now, how do you represent any vector in this space? Ah, we know that ! Its the linear combination of these basis vectors, $$r$$ and $$p$$.  Lets represent a general vector in this new space using notation $$u$$ instead of the $$v$$ that we used for $$H-W$$ space. Now any vector $$u$$ in $$R-P$$ space can be written as,

$$
u = c_1 * r + c_2 * p = c_1 * \begin{bmatrix}1\\3\end{bmatrix} + c_2 * \begin{bmatrix}2\\2\end{bmatrix}
$$

Also, to recall, we  represented a general vector $$v$$ in the $$H-W$$ space as

$$
v = c_1 * \begin{bmatrix}1\\0\end{bmatrix} + c_2 * \begin{bmatrix}0\\1\end{bmatrix}\\
$$

You can see that the only change that occurred when we transformed our vectors from the $$H-W$$ space into the $$R-P$$ space (derive heart rate and blood pressure from height and weight) is to the basis vectors! The scaling factors $$c_1$$ and $$c_2$$ still remain the same but the vectors that they scale (basis vectors) have changed. In $$H-W$$ space the basis vectors were orthonormal and hence the scaling factors were the vector components themselves. However, in the new $$R-P$$ space, the basis vectors $$r$$ and $$p$$ are no longer orthonormal and you need to perform multiplication with non-trivial numbers to get the transformed components. In summary, what I just explained is the matrix-vector multiplication as a process of transforming a vector from one space to the other space. The basis vectors of the transformed space are nothing but the columns of the transformation matrix  $$T$$. Here is how we write it.

$$
u = Tv = T \begin{bmatrix}v_h\\v_w\end{bmatrix} = \begin{bmatrix}1&2\\3&2\end{bmatrix}  \begin{bmatrix}v_h\\v_w\end{bmatrix} =  \begin{bmatrix}v_h+2v_w\\3v_h + 2v_w\end{bmatrix}
$$

Which is nothing but multiplying rows of the first matrix with the columns of the second and adding them together! Thus our height-weight vector $$v_1$$ get transformed into $$u_1$$ as below.

$$
u_1 = Tv_1 = \begin{bmatrix}1&2\\3&2\end{bmatrix}  \begin{bmatrix}1.5\\52\end{bmatrix} = \begin{bmatrix}105.5\\108.5\end{bmatrix}
$$

Now matrix-matrix multiplication is just transforming many vectors which are stacked side-by-side into a matrix. For example, matrix $$M$$ which has three such vectors, gets transformed into $$M^{'}$$

$$
M^{'} = TM =  \begin{bmatrix}1&2\\3&2\end{bmatrix} \begin{bmatrix}1.5&1.65&1.8\\52&65&80\end{bmatrix} = \begin{bmatrix}105.5&131.65&161.8\\108.5&134.95&165.4\end{bmatrix}
$$

The identity matrix $$B$$ defines the identity transformation. When applied, the transformed space will be same as the original space !

# Rank, inverse and determinant of a matrix

The example of transforming height and weight to deduce heart rate and blood pressure makes little sense, nonetheless it helped us to understand the matrix multiplication as applied to the real-world example. Now the question is, can we get back the original height and weight vector $$v$$ from the vector $$u$$? That is, can we inverse this transformation? We can do so, but it depends. We can reverse the transformation by multiplying the transformed vector by the inverse of transformation matrix if the inverse exists. We denote this as $$v = T^{-1}u$$.  What are the conditions for the existence of the inverse of the transformation matrix $$T$$? Before diving into that, we need to understand the rank of a matrix.

The rank of a matrix $$T$$ is simply the number of independent columns (vectors) of that matrix. In the 2-D case, how many column vectors of the matrix point in the different direction (or do not lie along same line). In our example, the transformation matrix $$T$$ has two columns which are independent. What happens if one of the columns of $$T$$ is dependent on the other? In this case, the space spanned by the columns of $$T$$ becomes 1-dimensional. Thus the rank of $$T$$ becomes 1 and every transformed vector $$u$$ lies along same line ! In other words, every vector $$v$$ in our 2-D space gets mapped on a single line as shown below. Thus there is no way that we can get back those original vectors $$v$$ from $$u$$. 

![image showing non-invertible transformation]({{ site.url }}/assets/matrix/non_invertible_transformation.png)

Matrix inverse is defined for square matrices ($$m = n$$). That is, the dimension of the original and transformed space are same. We need $$m$$ independent vectors as the basis of the transformed space and hence the inverse is possible if the rank is equal to $$m$$, that is the number of rows of the matrix. We will not discuss about how to compute the inverse of a matrix in this article. 

## Determinant

The absolute value of the determinant of a matrix $$T$$ is the area of the parallelogram spanned by its column vectors in the case of a $$2 \times 2$$ matrix  as shown below. For the 3-D space it is the volume and for $$m$$dimensional space it is hyper-volume.

![area spanned by 2 vectors in 2-D]({{ site.url }}/assets/matrix/determinant.png)

The determinant of a matrix
$$
T = \begin{bmatrix}h_1&w_1\\h_2&w_2\end{bmatrix}
$$

is thus the area spanned by vectors $$\begin{bmatrix}h_1\\w_1\end{bmatrix}$$ and $$\begin{bmatrix}h_2\\w_2\end{bmatrix}$$ and is equal to $$h_1*w_2 - h_2*w_1$$ as shown in the above figure. The determinant is a scalar quantity.

You can now figure out the way to compute the volume spanned by three 3-D vectors and you will end up with a formula for computing determinant of a $$3\times3$$ matrix.

If the columns of a 2$$\times$$2 transformation matrix are along same line, then the area spanned by them is zero. If any two columns of a $$3 \times 3$$ matrix are dependent then the volume spanned by them is zero or if all there of them are dependent then both the volume and the area spanned by them is zero. In such cases the determinant will be zero and there is no inverse for such matrices. Thus, the inverse of a transformation is only possible when the determinant of the transformation matrix is non-zero.

```

```

[

Another important use of the determinant is in normalizing the multivariate Gaussian probability density function. Thats why you see the determinant of the covariance matrix $$\Sigma$$ in the denominator of the density function.


$$
p_x(x_0...x_n) = \frac{1}{\sqrt{2\pi |\Sigma|}} . exp\left(-\frac{1}{2} (x -\mu )^{T}\Sigma^{-1}(x-\mu)\right)
$$

]

